{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6b3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\signavatars\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import sounddevice as sd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a96dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper...\n",
      "âœ“ Whisper loaded!\n",
      "\n",
      "Loading mBART50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\signavatars\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\anaconda3\\envs\\signavatars\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--facebook--mbart-large-50-many-to-many-mmt. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ mBART50 loaded!\n"
     ]
    }
   ],
   "source": [
    "# Whisper (Tamil ASR)\n",
    "print(\"Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"medium\").to(\"cuda\")\n",
    "print(\"âœ“ Whisper loaded!\\n\")\n",
    "\n",
    "# mBART50 (Tamil â†’ English)\n",
    "print(\"Loading mBART50...\")\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "    model_name,\n",
    "    src_lang=\"ta_IN\"\n",
    ")\n",
    "\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(\n",
    "    model_name\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"âœ“ mBART50 loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80192b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=\"tamil.wav\", duration=5, fs=16000):\n",
    "    os.makedirs(\"audio\", exist_ok=True)\n",
    "    path = os.path.join(\"audio\", filename)\n",
    "\n",
    "    print(f\"ðŸŽ¤ Recording for {duration} seconds...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "\n",
    "    sf.write(path, audio, fs)\n",
    "    print(f\"Saved to: {path}\")\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267fa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_tamil(audio_path):\n",
    "    audio, sr = sf.read(audio_path)\n",
    "\n",
    "    # mono\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    audio = audio.astype(np.float32)\n",
    "\n",
    "    # resample to 16k\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    print(\"ðŸ“ Transcribing Tamil...\")\n",
    "    result = whisper_model.transcribe(audio, fp16=True)\n",
    "    return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773b985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tamil_to_english(tamil_text):\n",
    "    print(\"ðŸŒ Translating Tamil â†’ English...\")\n",
    "\n",
    "    tokenizer.src_lang = \"ta_IN\"\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        tamil_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    generated = mbart_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "        num_beams=5,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    english = tokenizer.batch_decode(\n",
    "        generated,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "108563d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Recording for 5 seconds...\n",
      "Saved to: audio\\tamil.wav\n",
      "ðŸ“ Transcribing Tamil...\n",
      "\n",
      "Tamil Transcription:\n",
      " à®¨à®¾à®©à¯ à®®à®°à¯à®¤à¯à®¤à¯à®µà®®à¯ à®®à®£à¯ˆà®•à¯à®•à¯ à®šà¯†à®²à¯à®² à®µà¯‡à®£à¯à®Ÿà¯à®®à¯\n",
      "ðŸŒ Translating Tamil â†’ English...\n",
      "\n",
      "English Translation:\n",
      "I have to go to the medical center\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Record\n",
    "audio_file = record_audio(duration=5)\n",
    "\n",
    "# Step 2: Tamil STT\n",
    "tamil_text = transcribe_tamil(audio_file)\n",
    "print(\"\\nTamil Transcription:\")\n",
    "print(tamil_text)\n",
    "\n",
    "# Step 3: Translate\n",
    "english_text = translate_tamil_to_english(tamil_text)\n",
    "print(\"\\nEnglish Translation:\")\n",
    "print(english_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a14b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (3.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.32.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (3.1)\n",
      "Collecting tomli (from stanza)\n",
      "  Downloading tomli-2.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.4.1+cu121)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (2026.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.1/1.1 MB 2.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.3/1.1 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.4/1.1 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.6/1.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.0/1.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.0/1.1 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   ---------------------------------- ---- 532.5/608.4 kB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 608.4/608.4 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading tomli-2.4.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tomli, emoji, stanza\n",
      "Successfully installed emoji-2.15.0 stanza-1.10.1 tomli-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8098f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 6.69MB/s]                    \n",
      "2026-01-26 19:59:37 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2026-01-26 19:59:37 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/default.zip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 526M/526M [00:40<00:00, 12.9MB/s] \n",
      "2026-01-26 20:00:20 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\en\\default.zip\n",
      "2026-01-26 20:00:23 INFO: Finished downloading models and saved to C:\\Users\\HP\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246125cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 20:00:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 15.8MB/s]                    \n",
      "2026-01-26 20:00:41 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2026-01-26 20:00:41 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-01-26 20:00:42 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2026-01-26 20:00:42 INFO: Using device: cuda\n",
      "2026-01-26 20:00:42 INFO: Loading: tokenize\n",
      "2026-01-26 20:00:43 INFO: Loading: mwt\n",
      "2026-01-26 20:00:44 INFO: Loading: pos\n",
      "2026-01-26 20:00:49 INFO: Loading: lemma\n",
      "2026-01-26 20:00:49 INFO: Loading: depparse\n",
      "2026-01-26 20:00:50 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(\"en\", processors=\"tokenize,pos,lemma,depparse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc623e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_gloss_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    sentence = doc.sentences[0]\n",
    "\n",
    "    content_words = []\n",
    "    verbs = []\n",
    "\n",
    "    drop_pos = {\"AUX\", \"ADP\", \"DET\"}\n",
    "    drop_words = {\"the\", \"a\", \"an\", \"to\"}\n",
    "\n",
    "    for word in sentence.words:\n",
    "        pos = word.upos\n",
    "        lemma = word.lemma\n",
    "        text = word.text\n",
    "\n",
    "        # Skip junk words\n",
    "        if pos in drop_pos or text.lower() in drop_words:\n",
    "            continue\n",
    "\n",
    "        # VERBS â†’ base form + tense marking\n",
    "        if pos == \"VERB\":\n",
    "            lemma = lemma.upper()\n",
    "            # tense\n",
    "            if word.feats and \"Tense=Past\" in word.feats:\n",
    "                lemma += \"-PAST\"\n",
    "            verbs.append(lemma)\n",
    "        else:\n",
    "            content_words.append(text.upper())\n",
    "\n",
    "    # SOV order â†’ verbs moved to end\n",
    "    gloss = content_words + verbs\n",
    "    return \" \".join(gloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dec5c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I MEDICAL CENTER HAVE GO\n"
     ]
    }
   ],
   "source": [
    "gloss = english_to_gloss_stanza(english_text)\n",
    "print(gloss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signavatars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
