{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea6b3ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import sounddevice as sd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a96dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper...\n",
      "Whisper loaded!\n",
      "\n",
      "Loading mBART50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\envs\\signavatars\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART50 loaded!\n"
     ]
    }
   ],
   "source": [
    "# Whisper (Tamil ASR)\n",
    "print(\"Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"medium\").to(\"cuda\")\n",
    "print(\"Whisper loaded!\\n\")\n",
    "\n",
    "# mBART50 (Tamil → English)\n",
    "print(\"Loading mBART50...\")\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\n",
    "    model_name,\n",
    "    src_lang=\"ta_IN\"\n",
    ")\n",
    "\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(\n",
    "    model_name\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"mBART50 loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80192b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(filename=\"tamil.wav\", duration=5, fs=16000):\n",
    "    os.makedirs(\"audio\", exist_ok=True)\n",
    "    path = os.path.join(\"audio\", filename)\n",
    "\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype=\"float32\")\n",
    "    sd.wait()\n",
    "\n",
    "    sf.write(path, audio, fs)\n",
    "    print(f\"Saved to: {path}\")\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "267fa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_tamil(audio_path):\n",
    "    audio, sr = sf.read(audio_path)\n",
    "\n",
    "    # mono\n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    audio = audio.astype(np.float32)\n",
    "\n",
    "    # resample to 16k\n",
    "    if sr != 16000:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    print(\"Transcribing Tamil...\")\n",
    "    result = whisper_model.transcribe(audio, fp16=True)\n",
    "    return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "773b985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_tamil_to_english(tamil_text):\n",
    "    print(\"Translating Tamil → English...\")\n",
    "\n",
    "    tokenizer.src_lang = \"ta_IN\"\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        tamil_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    generated = mbart_model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "        num_beams=5,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    english = tokenizer.batch_decode(\n",
    "        generated,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "108563d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 5 seconds...\n",
      "Saved to: audio\\tamil.wav\n",
      "Transcribing Tamil...\n",
      "\n",
      "Tamil Transcription:\n",
      " நான் மருத்துவமனைக்கு செல்ல வேண்டும்\n",
      "Translating Tamil → English...\n",
      "\n",
      "English Translation:\n",
      "I have to go to hospital\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Record\n",
    "audio_file = record_audio(duration=5)\n",
    "\n",
    "# Step 2: Tamil STT\n",
    "tamil_text = transcribe_tamil(audio_file)\n",
    "print(\"\\nTamil Transcription:\")\n",
    "print(tamil_text)\n",
    "\n",
    "# Step 3: Translate\n",
    "english_text = translate_tamil_to_english(tamil_text)\n",
    "print(\"\\nEnglish Translation:\")\n",
    "print(english_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a14b6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (3.20.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.32.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (3.1)\n",
      "Requirement already satisfied: tomli in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.4.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (2.4.1+cu121)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from requests->stanza) (2026.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\signavatars\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8098f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 1.67MB/s]                    \n",
      "2026-01-28 21:07:07 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2026-01-28 21:07:07 INFO: Downloading default packages for language: en (English) ...\n",
      "2026-01-28 21:07:09 INFO: File exists: C:\\Users\\HP\\stanza_resources\\en\\default.zip\n",
      "2026-01-28 21:07:13 INFO: Finished downloading models and saved to C:\\Users\\HP\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "246125cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 21:07:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 434kB [00:00, 3.03MB/s]                    \n",
      "2026-01-28 21:07:17 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2026-01-28 21:07:18 WARNING: Language en package default expects mwt, which has been added\n",
      "2026-01-28 21:07:19 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2026-01-28 21:07:19 INFO: Using device: cuda\n",
      "2026-01-28 21:07:19 INFO: Loading: tokenize\n",
      "2026-01-28 21:07:19 INFO: Loading: mwt\n",
      "2026-01-28 21:07:19 INFO: Loading: pos\n",
      "2026-01-28 21:07:22 INFO: Loading: lemma\n",
      "2026-01-28 21:07:23 INFO: Loading: depparse\n",
      "2026-01-28 21:07:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(\"en\", processors=\"tokenize,pos,lemma,depparse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5bc623e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_gloss_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    sentence = doc.sentences[0]\n",
    "\n",
    "    content_words = []\n",
    "    verbs = []\n",
    "\n",
    "    drop_pos = {\"AUX\", \"ADP\", \"DET\"}\n",
    "    drop_words = {\"the\", \"a\", \"an\", \"to\"}\n",
    "\n",
    "    for word in sentence.words:\n",
    "        pos = word.upos\n",
    "        lemma = word.lemma\n",
    "        text = word.text\n",
    "\n",
    "        # Skip junk words\n",
    "        if pos in drop_pos or text.lower() in drop_words:\n",
    "            continue\n",
    "\n",
    "        # VERBS → base form + tense marking\n",
    "        if pos == \"VERB\":\n",
    "            lemma = lemma.upper()\n",
    "            # tense\n",
    "            if word.feats and \"Tense=Past\" in word.feats:\n",
    "                lemma += \"-PAST\"\n",
    "            verbs.append(lemma)\n",
    "        else:\n",
    "            content_words.append(text.upper())\n",
    "\n",
    "    # SOV order → verbs moved to end\n",
    "    gloss = content_words + verbs\n",
    "    return \" \".join(gloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dec5c3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HOSPITAL HAVE GO\n"
     ]
    }
   ],
   "source": [
    "gloss = english_to_gloss_stanza(english_text)\n",
    "print(gloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea07bd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gloss count: 2000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "WLASL_JSON = r\"D:\\projects\\spark\\WLASL\\WLASL_v0.3.json\"\n",
    "\n",
    "with open(WLASL_JSON, \"r\") as f:\n",
    "    wlasl_data = json.load(f)\n",
    "\n",
    "gloss_to_videos = {\n",
    "    entry[\"gloss\"]: [inst[\"video_id\"] for inst in entry[\"instances\"]]\n",
    "    for entry in wlasl_data\n",
    "}\n",
    "\n",
    "print(\"Loaded gloss count:\", len(gloss_to_videos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67303eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available OpenPose folders: 1739\n",
      "Sample folders:\n",
      "  -d5dN54tH2E_0-1-rgb_front\n",
      "  -d5dN54tH2E_1-1-rgb_front\n",
      "  -d5dN54tH2E_10-1-rgb_front\n",
      "  -d5dN54tH2E_11-1-rgb_front\n",
      "  -d5dN54tH2E_12-1-rgb_front\n"
     ]
    }
   ],
   "source": [
    "OPENPOSE_DIR = r\"D:\\projects\\spark\\openpose_output\\json\"\n",
    "\n",
    "available_folders = os.listdir(OPENPOSE_DIR)\n",
    "\n",
    "# Map numeric WLASL video_id to actual folder\n",
    "# Folder format: \"VIDEO_ID_INSTANCENUM-CAMNUM-rgb_front\"\n",
    "# We need to find which WLASL video_id corresponds to each folder\n",
    "\n",
    "video_id_to_folder = {}\n",
    "\n",
    "for entry in wlasl_data:\n",
    "    for instance in entry[\"instances\"]:\n",
    "        wlasl_vid_id = instance[\"video_id\"]\n",
    "        # Try to find a folder that might contain this video\n",
    "        # Folders have format like: \"-d5dN54tH2E_0-1-rgb_front\"\n",
    "        # The WLASL video_id might be embedded or we need to match by index\n",
    "        video_id_to_folder[wlasl_vid_id] = None\n",
    "\n",
    "# For now, let's use a simple approach: just use available folders\n",
    "print(f\"Available OpenPose folders: {len(available_folders)}\")\n",
    "print(f\"Sample folders:\")\n",
    "for folder in available_folders[:5]:\n",
    "    print(f\"  {folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "15d1a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openpose_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    people = data.get(\"people\", [])\n",
    "    if not people:\n",
    "        return None\n",
    "\n",
    "    p = people[0]\n",
    "    return {\n",
    "        \"pose\": p.get(\"pose_keypoints_2d\", []),\n",
    "        \"left\": p.get(\"hand_left_keypoints_2d\", []),\n",
    "        \"right\": p.get(\"hand_right_keypoints_2d\", [])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54585b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_sequence(folder):\n",
    "    frames = []\n",
    "    for file in sorted(os.listdir(folder)):\n",
    "        if file.endswith(\"_keypoints.json\"):\n",
    "            frame = load_openpose_json(os.path.join(folder, file))\n",
    "            if frame:\n",
    "                frames.append(frame)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0ae3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_keypoints(coords):\n",
    "    return [(coords[i], coords[i+1]) for i in range(0, len(coords), 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04150b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_avatar_frame(frame):\n",
    "    return {\n",
    "        \"pose\": flatten_keypoints(frame[\"pose\"]),\n",
    "        \"left_hand\": flatten_keypoints(frame[\"left\"]),\n",
    "        \"right_hand\": flatten_keypoints(frame[\"right\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2f2cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gloss_pose(gloss):\n",
    "    \"\"\"\n",
    "    Get pose sequence for a gloss.\n",
    "    Since we can't map WLASL video_ids to OpenPose folders directly,\n",
    "    use the first available OpenPose folder as a fallback.\n",
    "    \"\"\"\n",
    "    g = gloss.lower()\n",
    "\n",
    "    if g not in gloss_to_videos:\n",
    "        print(f\"Gloss '{g}' not in WLASL dictionary\")\n",
    "        return None\n",
    "\n",
    "    # Since we have limited OpenPose data (only ~171 samples),\n",
    "    # we'll just use any available sample\n",
    "    available_folders = os.listdir(OPENPOSE_DIR)\n",
    "    \n",
    "    if not available_folders:\n",
    "        print(f\"No OpenPose folders available\")\n",
    "        return None\n",
    "    \n",
    "    # Use the first available folder as demo data\n",
    "    folder_to_use = available_folders[0]\n",
    "    folder_path = os.path.join(OPENPOSE_DIR, folder_to_use)\n",
    "    \n",
    "    print(f\"Gloss '{g}' found in WLASL\")\n",
    "    print(f\"Using sample OpenPose data: {folder_to_use}\")\n",
    "    \n",
    "    poses = load_pose_sequence(folder_path)\n",
    "    if poses:\n",
    "        return poses\n",
    "    else:\n",
    "        print(f\"No keyframe data in {folder_to_use}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a671044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matched glosses: ['hospital', 'have', 'go']\n",
      "\n",
      "=== Trying gloss: 'hospital' ===\n",
      "Gloss 'hospital' found in WLASL\n",
      "Using sample OpenPose data: -d5dN54tH2E_0-1-rgb_front\n",
      "Poses returned: True\n",
      "Got 189 frames\n",
      "\n",
      "Final Avatar Frame:\n",
      "  Pose points: 25\n",
      "  Left hand: 21\n",
      "  Right hand: 21\n"
     ]
    }
   ],
   "source": [
    "gloss_words = [g.lower() for g in gloss.split()]\n",
    "valid_glosses = [g for g in gloss_words if g in gloss_to_videos and len(g) > 1]\n",
    "\n",
    "print(\"\\nMatched glosses:\", valid_glosses)\n",
    "\n",
    "poses = None\n",
    "avatar_frame = None\n",
    "\n",
    "# Try with the first matched gloss\n",
    "if valid_glosses:\n",
    "    g = valid_glosses[0]\n",
    "    print(f\"\\n=== Trying gloss: '{g}' ===\")\n",
    "    poses = get_gloss_pose(g)\n",
    "    print(f\"Poses returned: {poses is not None}\")\n",
    "    if poses and len(poses) > 0:\n",
    "        print(f\"Got {len(poses)} frames\")\n",
    "        avatar_frame = convert_to_avatar_frame(poses[0])\n",
    "        print(\"\\nFinal Avatar Frame:\")\n",
    "        print(f\"  Pose points: {len(avatar_frame['pose'])}\")\n",
    "        print(f\"  Left hand: {len(avatar_frame['left_hand'])}\")\n",
    "        print(f\"  Right hand: {len(avatar_frame['right_hand'])}\")\n",
    "else:\n",
    "    # Use a hardcoded gloss if nothing matched\n",
    "    print(\"\\nNo glosses matched, using fallback: 'hospital'\")\n",
    "    poses = get_gloss_pose(\"hospital\")\n",
    "    if poses:\n",
    "        avatar_frame = convert_to_avatar_frame(poses[0])\n",
    "        print(\"\\nAvatar Frame with fallback:\")\n",
    "        print(f\"  Pose points: {len(avatar_frame['pose'])}\")\n",
    "        print(f\"  Left hand: {len(avatar_frame['left_hand'])}\")\n",
    "        print(f\"  Right hand: {len(avatar_frame['right_hand'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88e6edff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BODY POSE (25 keypoints):\n",
      "   0. Nose            → (  666.54,   231.71)\n",
      "   1. Neck            → (  688.13,   370.77)\n",
      "   2. RShoulder       → (  572.63,   368.76)\n",
      "   3. RElbow          → (  511.80,   556.96)\n",
      "   4. RWrist          → (  552.97,   478.47)\n",
      "   5. LShoulder       → (  805.63,   378.57)\n",
      "   6. LElbow          → (  844.86,   564.72)\n",
      "   7. LWrist          → (  770.45,   543.19)\n",
      "   8. MidHip          → (  670.53,   666.66)\n",
      "   9. RHip            → (  598.02,   662.69)\n",
      "  10. RKnee           → (    0.00,     0.00)\n",
      "  11. RAnkle          → (    0.00,     0.00)\n",
      "  12. LHip            → (  743.01,   678.32)\n",
      "  13. LKnee           → (  790.04,   715.64)\n",
      "  14. LAnkle          → (    0.00,     0.00)\n",
      "  15. REye            → (  648.94,   212.17)\n",
      "  16. LEye            → (  693.98,   212.16)\n",
      "  17. REar            → (  623.53,   227.83)\n",
      "  18. LEar            → (  731.32,   241.46)\n",
      "  19. LBigToe         → (    0.00,     0.00)\n",
      "  20. LSmallToe       → (    0.00,     0.00)\n",
      "  21. LHeel           → (    0.00,     0.00)\n",
      "  22. RBigToe         → (    0.00,     0.00)\n",
      "  23. RSmallToe       → (    0.00,     0.00)\n",
      "  24. RHeel           → (    0.00,     0.00)\n",
      "\n",
      "LEFT HAND (21 keypoints):\n",
      "   0. (  773.81,   558.41)\n",
      "   1. (  787.07,   538.87)\n",
      "   2. (  798.93,   521.42)\n",
      "   3. (  808.71,   505.37)\n",
      "   4. (  805.22,   482.34)\n",
      "   5. (  765.43,   527.71)\n",
      "   6. (  746.59,   500.49)\n",
      "   7. (  737.52,   482.34)\n",
      "   8. (  727.75,   465.59)\n",
      "   9. (  748.68,   536.08)\n",
      "  10. (  726.35,   502.58)\n",
      "  11. (  721.47,   481.64)\n",
      "  12. (  713.79,   463.50)\n",
      "  13. (  736.82,   539.57)\n",
      "  14. (  713.09,   514.45)\n",
      "  15. (  715.18,   487.92)\n",
      "  16. (  715.88,   466.29)\n",
      "  17. (  726.35,   540.97)\n",
      "  18. (  710.30,   515.84)\n",
      "  19. (  710.30,   499.79)\n",
      "  20. (  711.70,   487.92)\n",
      "\n",
      "RIGHT HAND (21 keypoints):\n",
      "   0. (  557.84,   476.51)\n",
      "   1. (  549.86,   451.84)\n",
      "   2. (  547.69,   422.09)\n",
      "   3. (  552.04,   397.42)\n",
      "   4. (  548.41,   379.28)\n",
      "   5. (  577.43,   416.29)\n",
      "   6. (  591.95,   394.52)\n",
      "   7. (  599.93,   384.36)\n",
      "   8. (  605.00,   379.28)\n",
      "   9. (  593.40,   428.62)\n",
      "  10. (  609.36,   406.85)\n",
      "  11. (  604.28,   413.38)\n",
      "  12. (  598.48,   420.64)\n",
      "  13. (  605.00,   443.13)\n",
      "  14. (  616.61,   421.37)\n",
      "  15. (  612.99,   427.90)\n",
      "  16. (  605.73,   436.60)\n",
      "  17. (  614.44,   455.47)\n",
      "  18. (  626.77,   439.50)\n",
      "  19. (  621.69,   442.41)\n",
      "  20. (  618.07,   448.21)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print detailed coordinates\n",
    "if avatar_frame:\n",
    "    \n",
    "    print(\"\\nBODY POSE (25 keypoints):\")\n",
    "    pose_names = [\n",
    "        \"Nose\", \"Neck\", \"RShoulder\", \"RElbow\", \"RWrist\",\n",
    "        \"LShoulder\", \"LElbow\", \"LWrist\", \"MidHip\",\n",
    "        \"RHip\", \"RKnee\", \"RAnkle\", \"LHip\", \"LKnee\", \"LAnkle\",\n",
    "        \"REye\", \"LEye\", \"REar\", \"LEar\", \"LBigToe\",\n",
    "        \"LSmallToe\", \"LHeel\", \"RBigToe\", \"RSmallToe\", \"RHeel\"\n",
    "    ]\n",
    "    \n",
    "    for i, (x, y) in enumerate(avatar_frame['pose']):\n",
    "        name = pose_names[i] if i < len(pose_names) else f\"Point{i}\"\n",
    "        print(f\"  {i:2d}. {name:15s} → ({x:8.2f}, {y:8.2f})\")\n",
    "    \n",
    "    print(\"\\nLEFT HAND (21 keypoints):\")\n",
    "    for i, (x, y) in enumerate(avatar_frame['left_hand']):\n",
    "        print(f\"  {i:2d}. ({x:8.2f}, {y:8.2f})\")\n",
    "    \n",
    "    print(\"\\nRIGHT HAND (21 keypoints):\")\n",
    "    for i, (x, y) in enumerate(avatar_frame['right_hand']):\n",
    "        print(f\"  {i:2d}. ({x:8.2f}, {y:8.2f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"No avatar frame available to print coordinates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "444f7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Draw connections for BODY\n",
    "BODY_PAIRS = [\n",
    "    (0,1), (1,2), (2,3), (3,4),\n",
    "    (1,5), (5,6), (6,7),\n",
    "    (1,8), (8,9), (9,10),\n",
    "    (8,11), (11,12),\n",
    "]\n",
    "\n",
    "# Draw connections for HANDS (21 points each, following MediaPipe indexing)\n",
    "HAND_CONNECTIONS = [\n",
    "    (0,1),(1,2),(2,3),(3,4),\n",
    "    (0,5),(5,6),(6,7),(7,8),\n",
    "    (0,9),(9,10),(10,11),(11,12),\n",
    "    (0,13),(13,14),(14,15),(15,16),\n",
    "    (0,17),(17,18),(18,19),(19,20)\n",
    "]\n",
    "\n",
    "def draw_avatar_frame(frame_data, width=1280, height=720):\n",
    "    canvas = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Body\n",
    "    for a, b in BODY_PAIRS:\n",
    "        if a < len(frame_data[\"pose\"]) and b < len(frame_data[\"pose\"]):\n",
    "            x1, y1 = frame_data[\"pose\"][a]\n",
    "            x2, y2 = frame_data[\"pose\"][b]\n",
    "            if x1 != 0 and y1 != 0 and x2 != 0 and y2 != 0:\n",
    "                cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (0,255,0), 3)\n",
    "\n",
    "    # Left hand\n",
    "    for a, b in HAND_CONNECTIONS:\n",
    "        if a < len(frame_data[\"left_hand\"]) and b < len(frame_data[\"left_hand\"]):\n",
    "            x1, y1 = frame_data[\"left_hand\"][a]\n",
    "            x2, y2 = frame_data[\"left_hand\"][b]\n",
    "            cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (255,0,0), 2)\n",
    "\n",
    "    # Right hand\n",
    "    for a, b in HAND_CONNECTIONS:\n",
    "        if a < len(frame_data[\"right_hand\"]) and b < len(frame_data[\"right_hand\"]):\n",
    "            x1, y1 = frame_data[\"right_hand\"][a]\n",
    "            x2, y2 = frame_data[\"right_hand\"][b]\n",
    "            cv2.line(canvas, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)\n",
    "\n",
    "    return canvas\n",
    "\n",
    "def animate_sequence(poses):\n",
    "    print(f\"Animating {len(poses)} frames...\")\n",
    "    for frame in poses:\n",
    "        avatar = convert_to_avatar_frame(frame)\n",
    "        img = draw_avatar_frame(avatar)\n",
    "\n",
    "        cv2.imshow(\"Sign Avatar\", img)\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e10cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animating 189 frames...\n"
     ]
    }
   ],
   "source": [
    "animate_sequence(poses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4da955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def animate_sequence_notebook(poses, delay=0.03):\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    for frame in poses:\n",
    "        avatar = convert_to_avatar_frame(frame)\n",
    "\n",
    "        # Draw image\n",
    "        img = draw_avatar_frame(avatar)\n",
    "        plt.imshow(img[:,:,::-1])\n",
    "        plt.axis('off')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(plt.gcf())\n",
    "        plt.pause(delay)\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "825152fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animate_sequence_notebook(poses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signavatars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
